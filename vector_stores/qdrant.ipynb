{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/qdrant.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import qdrant_client\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import local_secrets as secrets\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Qdrant, Pinecone\n",
    "from langchain.document_loaders import TextLoader\n",
    "from llama_index.readers.qdrant import QdrantReader\n",
    "from llama_index import GPTListIndex\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "import pinecone\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.212\n"
     ]
    }
   ],
   "source": [
    "os.environ['OPENAI_API_KEY'] = secrets.techstyle_openai_key\n",
    "os.environ['PINECONE_API_KEY'] = secrets.techstyle_pinecone_api_key\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = TextLoader('../state_of_the_union.txt')\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "qdrant = Qdrant.from_documents(docs, embeddings, url='http://localhost:6333', collection_name=\"state_of_the_union\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader = QdrantReader(host=\"localhost\")\n",
    "query_vector=[0.3, 0.7]*768\n",
    "documents = reader.load_data(collection_name=\"github_llama_index\", query_vector=query_vector, limit=5)\n",
    "index = GPTListIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file_name': 'mock_embed_model.py',\n",
       "  'file_path': 'llama_index/token_counter/mock_embed_model.py'},\n",
       " {'file_name': 'sql_query.py',\n",
       "  'file_path': 'llama_index/indices/struct_store/sql_query.py'},\n",
       " {'file_name': 'base.py',\n",
       "  'file_path': 'llama_index/indices/struct_store/base.py'},\n",
       " {'file_name': 'pandas_query.py',\n",
       "  'file_path': 'llama_index/indices/struct_store/pandas_query.py'},\n",
       " {'file_name': 'schema.py',\n",
       "  'file_path': 'llama_index/indices/query/schema.py'}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.extra_info for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents[0].get_embedding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://qdrant.tech/articles/langchain-integration/\n",
    "os.environ['OPENAI_API_KEY'] = secrets.techstyle_openai_key\n",
    "llm = ChatOpenAI()\n",
    "# pinecone\n",
    "client = pinecone.init(api_key=secrets.techstyle_pinecone_api_key, environment='us-east-1-aws')\n",
    "retriever = Pinecone(index=pinecone.Index('ssk'), embedding_function=OpenAIEmbeddings, text_key='text').as_retriever()\n",
    "chain = load_qa_chain(llm=llm, chain_type='stuff') \n",
    "\n",
    "question = 'What are the classes in the llama_index package?'\n",
    "question = 'How is the llama_index Node class used'\n",
    "question = 'Show an example of how to use the llama_index Node class'\n",
    "question = 'How can several llama_indexes be composed?'\n",
    "question = 'Explain llama_index nodes'\n",
    "# documents = retriever.get_relevant_documents(question)\n",
    "#chain.run(input_documents=documents, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceaa99ebebc544669bc69913d6b08764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value=''), Button(description='Chat', style=ButtonStyle()), Output()))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        documents = retriever.get_relevant_documents(question.value)\n",
    "        print(chain.run(input_documents=documents, question=question.value))\n",
    "output = widgets.Output()\n",
    "question = widgets.Text()\n",
    "button = widgets.Button(description='Chat')\n",
    "button.on_click(on_button_clicked)\n",
    "widgets.VBox([question, button, output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['page_content'], output_parser=None, partial_variables={}, template='{page_content}', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(chain.document_prompt)\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'github-llama-index': {'vector_count': 335}},\n",
      " 'total_vector_count': 335}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.Index('ssk')\n",
    "print(index.describe_index_stats())\n",
    "vectorstore = Pinecone.from_existing_index(index_name='ssk', embedding=OpenAIEmbeddings(), namespace='github-llama-index')\n",
    "documents = vectorstore.similarity_search('How can several llama_indexes be composed?')\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is an example of how several LlamaIndexes can be composed using the `CompositeIndex` class:\\n\\n```python\\nfrom llama_index import LlamaIndex, CompositeIndex\\n\\n# Create individual indexes\\nlist_index = LlamaIndex(\"list_index\")\\nvector_store_index = LlamaIndex(\"vector_store_index\")\\ntree_index = LlamaIndex(\"tree_index\")\\nkeyword_table_index = LlamaIndex(\"keyword_table_index\")\\n\\n# Add indexes to the composite index\\ncomposite_index = CompositeIndex()\\ncomposite_index.add_index(list_index)\\ncomposite_index.add_index(vector_store_index)\\ncomposite_index.add_index(tree_index)\\ncomposite_index.add_index(keyword_table_index)\\n```\\n\\nIn this example, we create four individual LlamaIndexes: `list_index`, `vector_store_index`, `tree_index`, and `keyword_table_index`. These indexes can be of different types, such as the List Index, Vector Store Index, Tree Index, or Keyword Table Index.\\n\\nThen, we create a `CompositeIndex` and add the individual indexes to it using the `add_index` method. This creates a composite index that includes all the functionality of the individual indexes. Queries can be performed on the composite index, and it will internally utilize the appropriate index based on the query parameters.\\n\\nBy composing multiple indexes together, you can leverage the strengths of each index type and create a more comprehensive search system.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='gpt-3.5-turbo-16k')\n",
    "retriever = Pinecone.from_existing_index(index_name='ssk', embedding=OpenAIEmbeddings(), namespace='github-llama-index').as_retriever()\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_reduce\", retriever=retriever, return_source_documents=True)\n",
    "result = qa({\"query\": 'How can several llama_indexes be composed? Show an example.'})\n",
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Qdrant(client=client, collection_name='github_llama_index', embeddings=OpenAIEmbeddings(), content_payload_key='text').as_retriever()\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_reduce\", retriever=retriever, return_source_documents=True)\n",
    "result = qa({\"query\": 'How can several llama_indexes be composed? Show an example.'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.212\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
