{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/qdrant.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import qdrant_client\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import local_secrets as secrets\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Qdrant, Pinecone\n",
    "from langchain.document_loaders import TextLoader\n",
    "from llama_index.readers.qdrant import QdrantReader\n",
    "from llama_index import GPTListIndex\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "import pinecone\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.212\n"
     ]
    }
   ],
   "source": [
    "os.environ['OPENAI_API_KEY'] = secrets.techstyle_openai_key\n",
    "os.environ['PINECONE_API_KEY'] = secrets.techstyle_pinecone_api_key\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = TextLoader('../state_of_the_union.txt')\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "qdrant = Qdrant.from_documents(docs, embeddings, url='http://localhost:6333', collection_name=\"state_of_the_union\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['062f83e9a6a7963d21c71b6e9a79dd698aa7a9a9',\n",
       " '2f13b031f5179ebb3a591406e5e6a6ef5e2769de',\n",
       " 'f5f98621416fb23d459bd8cdc6f41ae8f8ea8a53',\n",
       " 'fff1e849717211a1bcc1d0154ecc6543bd646e5f',\n",
       " '103df1e6df518549cd29d0129bcf6217e8e9b0eb']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = QdrantReader(host=\"localhost\")\n",
    "query_vector=[0.3, 0.7]*768\n",
    "documents = reader.load_data(collection_name=\"github\", query_vector=query_vector, limit=5)\n",
    "index = GPTListIndex.from_documents(documents)\n",
    "print(len(documents[0].get_embedding()))\n",
    "[doc.doc_id for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://qdrant.tech/articles/langchain-integration/\n",
    "os.environ['OPENAI_API_KEY'] = secrets.techstyle_openai_key\n",
    "llm = ChatOpenAI()\n",
    "# pinecone\n",
    "client = pinecone.init(api_key=secrets.techstyle_pinecone_api_key, environment='us-east-1-aws')\n",
    "retriever = Pinecone(index=pinecone.Index('ssk'), embedding_function=OpenAIEmbeddings, text_key='text').as_retriever()\n",
    "chain = load_qa_chain(llm=llm, chain_type='stuff') \n",
    "\n",
    "question = 'What are the classes in the llama_index package?'\n",
    "question = 'How is the llama_index Node class used'\n",
    "question = 'Show an example of how to use the llama_index Node class'\n",
    "question = 'How can several llama_indexes be composed?'\n",
    "question = 'Explain llama_index nodes'\n",
    "# documents = retriever.get_relevant_documents(question)\n",
    "#chain.run(input_documents=documents, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceaa99ebebc544669bc69913d6b08764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value=''), Button(description='Chat', style=ButtonStyle()), Output()))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        documents = retriever.get_relevant_documents(question.value)\n",
    "        print(chain.run(input_documents=documents, question=question.value))\n",
    "output = widgets.Output()\n",
    "question = widgets.Text()\n",
    "button = widgets.Button(description='Chat')\n",
    "button.on_click(on_button_clicked)\n",
    "widgets.VBox([question, button, output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['page_content'], output_parser=None, partial_variables={}, template='{page_content}', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(chain.document_prompt)\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Composability\\n\\n\\nLlamaIndex offers **composability** of your indices, meaning that you can build indices on top of other indices. This allows you to more effectively index your entire document tree in order to feed custom knowledge to GPT.\\n\\nComposability allows you to to define lower-level indices for each document, and higher-order indices over a collection of documents. To see how this works, imagine defining 1) a tree index for the text within each document, and 2) a list index over each tree index (one document) within your collection.\\n\\n### Defining Subindices\\nTo see how this works, imagine you have 3 documents: `doc1`, `doc2`, and `doc3`.\\n\\n```python\\ndoc1 = SimpleDirectoryReader(\\'data1\\').load_data()\\ndoc2 = SimpleDirectoryReader(\\'data2\\').load_data()\\ndoc3 = SimpleDirectoryReader(\\'data3\\').load_data()\\n```\\n\\n![](/_static/composability/diagram_b0.png)\\n\\nNow let\\'s define a tree index for each document. In Python, we have:\\n\\n```python\\nindex1 = GPTTreeIndex.from_documents(doc1)\\nindex2 = GPTTreeIndex.from_documents(doc2)\\nindex3 = GPTTreeIndex.from_documents(doc3)\\n```\\n\\n![](/_static/composability/diagram_b1.png)\\n\\n### Defining Summary Text\\n\\nYou then need to explicitly define *summary text* for each subindex. This allows  \\nthe subindices to be used as Documents for higher-level indices.\\n\\n```python\\nindex1_summary = \"<summary1>\"\\nindex2_summary = \"<summary2>\"\\nindex3_summary = \"<summary3>\"\\n```\\n\\nYou may choose to manually specify the summary text, or use LlamaIndex itself to generate\\na summary, for instance with the following:\\n\\n```python\\nsummary = index1.query(\\n    \"What is a summary of this document?\", retriever_mode=\"all_leaf\"\\n)\\nindex1_summary = str(summary)\\n```\\n\\n**If specified**, this summary text for each subindex can be used to refine the answer during query-time. \\n\\n### Creating a Graph with a Top-Level Index\\n\\nWe can then create a graph with a list index on top of these 3 tree indices:\\nWe can query, save, and load the graph to/from disk as any other index.\\n\\n```python\\nfrom llama_index.indices.composability import ComposableGraph\\n\\ngraph = ComposableGraph.from_indices(\\n    GPTListIndex,\\n    [index1, index2, index3],\\n    index_summaries=[index1_summary, index2_summary, index3_summary],\\n)\\n\\n```\\n\\n![](/_static/composability/diagram.png)\\n\\n\\n### Querying the Graph\\n\\nDuring a query, we would start with the top-level list index. Each node in the list corresponds to an underlying tree index. \\nThe query will be executed recursively, starting from the root index, then the sub-indices.\\nThe default query engine for each index is called under the hood (i.e. `index.as_query_engine()`), unless otherwise configured by passing `custom_query_engines` to the `ComposableGraphQueryEngine`.\\nBelow we show an example that configure the tree index retrievers to use `child_branch_factor=2` (instead of the default `child_branch_factor=1`).\\n\\n\\nMore detail on how to configure `ComposableGraphQueryEngine` can be found [here](/reference/query/query_engines/graph_query_engine.rst).\\n\\n\\n```python\\n# set custom retrievers. An example is provided below\\ncustom_query_engines = {\\n    index.index_id: index.as_query_engine(\\n        child_branch_factor=2\\n    ) \\n    for index in [index1, index2, index3]\\n}\\nquery_engine = graph.as_query_engine(\\n    custom_query_engines=custom_query_engines\\n)\\nresponse = query_engine.query(\"Where did the author grow up?\")\\n```\\n\\n> Note that specifying custom retriever for index by id\\n> might require you to inspect e.g., `index1.index_struct.index_id`.\\n> Alternatively, you can explicitly set it as follows:\\n```python\\nindex1.index_struct.index_id = \"<index_id_1>\"\\nindex2.index_struct.index_id = \"<index_id_2>\"\\nindex3.index_struct.index_id = \"<index_id_3>\"\\n```\\n\\n![](/_static/composability/diagram_q1.png)\\n\\nSo within a node, instead of fetching the text, we would recursively query the stored tree index to retrieve our answer.\\n\\n![](/_static/composability/diagram_q2.png)\\n\\nNOTE: You can stack indices as many times as you want, depending on the hierarchies of your knowledge base! \\n\\n\\nWe can take a look at a code example below as well. We first build two tree indices, one over the Wikipedia NYC page, and the other over Paul Graham\\'s essay. We then define a keyword extractor index over the two tree indices.\\n\\n[Here is an example notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/composable_indices/ComposableIndices.ipynb).\\n\\n\\n```{toctree}\\n---\\ncaption: Examples\\nmaxdepth: 1\\n---\\n../../examples/composable_indices/ComposableIndices-Prior.ipynb\\n../../examples/composable_indices/ComposableIndices-Weaviate.ipynb\\n../../examples/composable_indices/ComposableIndices.ipynb\\n```', metadata={}),\n",
       " Document(page_content='# How Each Index Works\\n\\nThis guide describes how each index works with diagrams. We also visually highlight our \"Response Synthesis\" modes.\\n\\nSome terminology:\\n- **Node**: Corresponds to a chunk of text from a Document. LlamaIndex takes in Document objects and internally parses/chunks them into Node objects.\\n- **Response Synthesis**: Our module which synthesizes a response given the retrieved Node. You can see how to \\n    [specify different response modes](setting-response-mode) here. \\n    See below for an illustration of how each response mode works.\\n\\n## List Index\\n\\nThe list index simply stores Nodes as a sequential chain.\\n\\n![](/_static/indices/list.png)\\n\\n### Querying\\n\\nDuring query time, if no other query parameters are specified, LlamaIndex simply loads all Nodes in the list into\\nour Reponse Synthesis module.\\n\\n![](/_static/indices/list_query.png)\\n\\nThe list index does offer numerous ways of querying a list index, from an embedding-based query which \\nwill fetch the top-k neighbors, or with the addition of a keyword filter, as seen below:\\n\\n![](/_static/indices/list_filter_query.png)\\n\\n\\n## Vector Store Index\\n\\nThe vector store index stores each Node and a corresponding embedding in a [Vector Store](vector-store-index).\\n\\n![](/_static/indices/vector_store.png)\\n\\n### Querying\\n\\nQuerying a vector store index involves fetching the top-k most similar Nodes, and passing\\nthose into our Response Synthesis module.\\n\\n![](/_static/indices/vector_store_query.png)\\n\\n## Tree Index\\n\\nThe tree index builds a hierarchical tree from a set of Nodes (which become leaf nodes in this tree).\\n\\n![](/_static/indices/tree.png)\\n\\n### Querying\\n\\nQuerying a tree index involves traversing from root nodes down \\nto leaf nodes. By default, (`child_branch_factor=1`), a query\\nchooses one child node given a parent node. If `child_branch_factor=2`, a query\\nchooses two child nodes per parent.\\n\\n![](/_static/indices/tree_query.png)\\n\\n## Keyword Table Index\\n\\nThe keyword table index extracts keywords from each Node and builds a mapping from \\neach keyword to the corresponding Nodes of that keyword.\\n\\n![](/_static/indices/keyword.png)\\n\\n### Querying\\n\\nDuring query time, we extract relevant keywords from the query, and match those with pre-extracted\\nNode keywords to fetch the corresponding Nodes. The extracted Nodes are passed to our \\nResponse Synthesis module.\\n\\n![](/_static/indices/keyword_query.png)\\n\\n## Response Synthesis\\n\\nLlamaIndex offers different methods of synthesizing a response. The way to toggle this can be found in our \\n[Usage Pattern Guide](setting-response-mode). Below, we visually highlight how each response mode works.\\n\\n### Create and Refine\\n\\nCreate and refine is an iterative way of generating a response. We first use the context in the first node, along\\nwith the query, to generate an initial answer. We then pass this answer, the query, and the context of the second node\\nas input into a \"refine prompt\" to generate a refined answer. We refine through N-1 nodes, where N is the total \\nnumber of nodes.\\n\\n![](/_static/indices/create_and_refine.png)\\n\\n### Tree Summarize\\n\\nTree summarize is another way of generating a response. We essentially build a tree index\\nover the set of candidate nodes, with a *summary prompt* seeded with the query. The tree\\nis built in a bottoms-up fashion, and in the end the root node is returned as the response.\\n\\n![](/_static/indices/tree_summarize.png)\\n', metadata={}),\n",
       " Document(page_content='# How Each Index Works\\n\\nThis guide describes how each index works with diagrams. \\n\\nSome terminology:\\n- **Node**: Corresponds to a chunk of text from a Document. LlamaIndex takes in Document objects and internally parses/chunks them into Node objects.\\n- **Response Synthesis**: Our module which synthesizes a response given the retrieved Node. You can see how to \\n    [specify different response modes](setting-response-mode) here. \\n\\n## List Index\\n\\nThe list index simply stores Nodes as a sequential chain.\\n\\n![](/_static/indices/list.png)\\n\\n### Querying\\n\\nDuring query time, if no other query parameters are specified, LlamaIndex simply loads all Nodes in the list into\\nour Response Synthesis module.\\n\\n![](/_static/indices/list_query.png)\\n\\nThe list index does offer numerous ways of querying a list index, from an embedding-based query which \\nwill fetch the top-k neighbors, or with the addition of a keyword filter, as seen below:\\n\\n![](/_static/indices/list_filter_query.png)\\n\\n\\n## Vector Store Index\\n\\nThe vector store index stores each Node and a corresponding embedding in a [Vector Store](vector-store-index).\\n\\n![](/_static/indices/vector_store.png)\\n\\n### Querying\\n\\nQuerying a vector store index involves fetching the top-k most similar Nodes, and passing\\nthose into our Response Synthesis module.\\n\\n![](/_static/indices/vector_store_query.png)\\n\\n## Tree Index\\n\\nThe tree index builds a hierarchical tree from a set of Nodes (which become leaf nodes in this tree).\\n\\n![](/_static/indices/tree.png)\\n\\n### Querying\\n\\nQuerying a tree index involves traversing from root nodes down \\nto leaf nodes. By default, (`child_branch_factor=1`), a query\\nchooses one child node given a parent node. If `child_branch_factor=2`, a query\\nchooses two child nodes per level.\\n\\n![](/_static/indices/tree_query.png)\\n\\n## Keyword Table Index\\n\\nThe keyword table index extracts keywords from each Node and builds a mapping from \\neach keyword to the corresponding Nodes of that keyword.\\n\\n![](/_static/indices/keyword.png)\\n\\n### Querying\\n\\nDuring query time, we extract relevant keywords from the query, and match those with pre-extracted\\nNode keywords to fetch the corresponding Nodes. The extracted Nodes are passed to our \\nResponse Synthesis module.\\n\\n![](/_static/indices/keyword_query.png)\\n', metadata={}),\n",
       " Document(page_content='# Queries over your Data\\n\\nAt a high-level, LlamaIndex gives you the ability to query your data for any downstream LLM use case,\\nwhether it\\'s question-answering, summarization, or a component in a chatbot.\\n\\nThis section describes the different ways you can query your data with LlamaIndex, roughly in order\\nof simplest (top-k semantic search), to more advanced capabilities.\\n\\n### Semantic Search \\n\\nThe most basic example usage of LlamaIndex is through semantic search. We provide\\na simple in-memory vector store for you to get started, but you can also choose\\nto use any one of our [vector store integrations](/how_to/integrations/vector_stores.md):\\n\\n```python\\nfrom llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\\ndocuments = SimpleDirectoryReader(\\'data\\').load_data()\\nindex = GPTVectorStoreIndex.from_documents(documents)\\nquery_engine = index.as_query_engine()\\nresponse = query_engine.query(\"What did the author do growing up?\")\\nprint(response)\\n\\n```\\n\\nRelevant Resources:\\n- [Quickstart](/getting_started/starter_example.md)\\n- [Example](../examples/vector_stores/SimpleIndexDemo.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/tree/main/docs/examples/vector_stores/SimpleIndexDemo.ipynb))\\n\\n\\n### Summarization\\n\\nA summarization query requires the LLM to iterate through many if not most documents in order to synthesize an answer.\\nFor instance, a summarization query could look like one of the following: \\n- \"What is a summary of this collection of text?\"\\n- \"Give me a summary of person X\\'s experience with the company.\"\\n\\nIn general, a list index would be suited for this use case. A list index by default goes through all the data.\\n\\nEmpirically, setting `response_mode=\"tree_summarize\"` also leads to better summarization results.\\n\\n```python\\nindex = GPTListIndex.from_documents(documents)\\n\\nquery_engine = index.as_query_engine(\\n    response_mode=\"tree_summarize\"\\n)\\nresponse = query_engine.query(\"<summarization_query>\")\\n```\\n\\n### Queries over Structured Data\\n\\nLlamaIndex supports queries over structured data, whether that\\'s a Pandas DataFrame or a SQL Database.\\n\\nHere are some relevant resources:\\n\\n**Guides**\\n\\n- [Guide on Text-to-SQL](/guides/tutorials/sql_guide.md)\\n\\n**Examples**\\n- [SQL Demo 1](../examples/index_structs/struct_indices/SQLIndexDemo.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/index_structs/struct_indices/SQLIndexDemo.ipynb))\\n- [SQL Demo 2 (Context)](../examples/index_structs/struct_indices/SQLIndexDemo-Context.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/index_structs/struct_indices/SQLIndexDemo-Context.ipynb))\\n- [SQL Demo 3 (Big tables)](../examples/index_structs/struct_indices//SQLIndexDemo-ManyTables.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/index_structs/struct_indices/SQLIndexDemo-ManyTables.ipynb))\\n- [Pandas Demo](../examples/index_structs/struct_indices/PandasIndexDemo.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/index_structs/struct_indices/PandasIndexDemo.ipynb))\\n\\n\\n### Synthesis over Heterogeneous Data\\n\\nLlamaIndex supports synthesizing across heterogeneous data sources. This can be done by composing a graph over your existing data.\\nSpecifically, compose a list index over your subindices. A list index inherently combines information for each node; therefore\\nit can synthesize information across your heterogeneous data sources.\\n\\n```python\\nfrom llama_index import GPTVectorStoreIndex, GPTListIndex\\nfrom llama_index.indices.composability import ComposableGraph\\n\\nindex1 = GPTVectorStoreIndex.from_documents(notion_docs)\\nindex2 = GPTVectorStoreIndex.from_documents(slack_docs)\\n\\ngraph = ComposableGraph.from_indices(GPTListIndex, [index1, index2], index_summaries=[\"summary1\", \"summary2\"])\\nquery_engine = graph.as_query_engine()\\nresponse = query_engine.query(\"<query_str>\")\\n\\n```\\n\\nHere are some relevant resources:\\n- [Composability](/how_to/index_structs/composability.md)\\n- [City Analysis](../examples/composable_indices/city_analysis/PineconeDemo-CityAnalysis.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/composable_indices/city_analysis/PineconeDemo-CityAnalysis.ipynb))\\n\\n\\n\\n### Routing over Heterogeneous Data\\n\\nLlamaIndex also supports routing over heterogeneous data sources with `RouterQueryEngine` - for instance, if you want to \"route\" a query to an \\nunderlying Document or a sub-index.\\n\\n\\nTo do this, first build the sub-indices over different data sources.\\nThen construct the corresponding query engines, and give each query engine a description to obtain a `QueryEngineTool`.\\n\\n```python\\nfrom llama_index import GPTTreeIndex, GPTVectorStoreIndex\\nfrom llama_index.tools import QueryEngineTool\\n\\n...\\n\\n# define sub-indices\\nindex1 = GPTVectorStoreIndex.from_documents(notion_docs)\\nindex2 = GPTVectorStoreIndex.from_documents(slack_docs)\\n\\n# define query engines and tools\\ntool1 = QueryEngineTool.from_defaults(\\n    query_engine=index1.as_query_engine(), \\n    description=\"Use this query engine to do...\",\\n)\\ntool2 = QueryEngineTool.from_defaults(\\n    query_engine=index2.as_query_engine(), \\n    description=\"Use this query engine for something else...\",\\n)\\n```\\n\\nThen, we define a `RouterQueryEngine` over them.\\nBy default, this uses a `LLMSingleSelector` as the router, which uses the LLM to choose the best sub-index to router the query to, given the descriptions.\\n\\n```python\\nfrom llama_index.query_engine import RouterQueryEngine\\n\\nquery_engine = RouterQueryEngine.from_defaults(\\n    query_engine_tools=[tool1, tool2]\\n)\\n\\nresponse = query_engine.query(\\n    \"In Notion, give me a summary of the product roadmap.\"\\n)\\n\\n```\\n\\nHere are some relevant resources:\\n- [Router Query Engine Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/query_engine/RouterQueryEngine.ipynb).\\n- [City Analysis Unified Query Interface](../examples/composable_indices/city_analysis/City_Analysis-Unified-Query.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/composable_indices/city_analysis/PineconeDemo-CityAnalysis.ipynb))\\n\\n\\n### Compare/Contrast Queries\\n\\nLlamaIndex can support compare/contrast queries as well. It can do this in the following fashion:\\n- Composing a graph over your data\\n- Adding in query transformations.\\n\\n\\nYou can perform compare/contrast queries by just composing a graph over your data.\\n\\nHere are some relevant resources:\\n- [Composability](/how_to/index_structs/composability.md)\\n- [SEC 10-k Analysis Example notebook](https://colab.research.google.com/drive/1uL1TdMbR4kqa0Ksrd_Of_jWSxWt1ia7o?usp=sharing).\\n\\n\\nYou can also perform compare/contrast queries with a **query transformation** module.\\n\\n```python\\nfrom llama_index.indices.query.query_transform.base import DecomposeQueryTransform\\ndecompose_transform = DecomposeQueryTransform(\\n    llm_predictor_chatgpt, verbose=True\\n)\\n```\\n\\nThis module will help break down a complex query into a simpler one over your existing index structure.\\n\\nHere are some relevant resources:\\n- [Query Transformations](/how_to/query/query_transformations.md)\\n- [City Analysis Compare/Contrast Example](../examples//composable_indices/city_analysis/City_Analysis-Decompose.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/composable_indices/city_analysis/City_Analysis-Decompose.ipynb))\\n\\n### Multi-Step Queries\\n\\nLlamaIndex can also support multi-step queries. Given a complex query, break it down into subquestions.\\n\\nFor instance, given a question \"Who was in the first batch of the accelerator program the author started?\",\\nthe module will first decompose the query into a simpler initial question \"What was the accelerator program the author started?\",\\nquery the index, and then ask followup questions.\\n\\nHere are some relevant resources:\\n- [Query Transformations](/how_to/query/query_transformations.md)\\n- [Multi-Step Query Decomposition](../examples/query_transformations/HyDEQueryTransformDemo.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/query_transformations/HyDEQueryTransformDemo.ipynb))\\n\\n\\n\\n\\n', metadata={})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index = ('ssk')\n",
    "#print(index.describe_index_stats())\n",
    "vectorstore = Qdrant(client=qdrant_client.QdrantClient(url='http://localhost:6333'), collection_name='github', embeddings=OpenAIEmbeddings(), content_payload_key='text',metadata_payload_key=\"Payload\",)\n",
    "documents = vectorstore.similarity_search('How can several llama_indexes be composed?')\n",
    "[doc for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'github-llama-index': {'vector_count': 335}},\n",
      " 'total_vector_count': 335}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'doc_id': 'cc1c9dba6db2572cc3b8db8427880ca8a7240366',\n",
       "  'document_id': 'cc1c9dba6db2572cc3b8db8427880ca8a7240366',\n",
       "  'file_name': 'composability.md',\n",
       "  'file_path': 'docs/how_to/index_structs/composability.md',\n",
       "  'node_info': '{\"_node_type\": \"1\"}',\n",
       "  'ref_doc_id': 'cc1c9dba6db2572cc3b8db8427880ca8a7240366',\n",
       "  'relationships': '{\"1\": \"cc1c9dba6db2572cc3b8db8427880ca8a7240366\"}'},\n",
       " {'doc_id': 'e62fe521e0df705f5651ca17f5656ca3b13cd533',\n",
       "  'document_id': 'e62fe521e0df705f5651ca17f5656ca3b13cd533',\n",
       "  'file_name': 'index_guide.md',\n",
       "  'file_path': 'examples/multimodal/data/llama/index_guide.md',\n",
       "  'node_info': '{\"_node_type\": \"1\"}',\n",
       "  'ref_doc_id': 'e62fe521e0df705f5651ca17f5656ca3b13cd533',\n",
       "  'relationships': '{\"1\": \"e62fe521e0df705f5651ca17f5656ca3b13cd533\"}'},\n",
       " {'doc_id': 'c24042be22bf069bc8c283c253fe4c2fcdca3a9a',\n",
       "  'document_id': 'c24042be22bf069bc8c283c253fe4c2fcdca3a9a',\n",
       "  'file_name': 'index_guide.md',\n",
       "  'file_path': 'docs/guides/primer/index_guide.md',\n",
       "  'node_info': '{\"_node_type\": \"1\"}',\n",
       "  'ref_doc_id': 'c24042be22bf069bc8c283c253fe4c2fcdca3a9a',\n",
       "  'relationships': '{\"1\": \"c24042be22bf069bc8c283c253fe4c2fcdca3a9a\"}'},\n",
       " {'doc_id': 'ea9e88f3a6c897271dcf5506059799dfc02b2f3c',\n",
       "  'document_id': 'ea9e88f3a6c897271dcf5506059799dfc02b2f3c',\n",
       "  'file_name': 'apps.md',\n",
       "  'file_path': 'docs/use_cases/apps.md',\n",
       "  'node_info': '{\"_node_type\": \"1\"}',\n",
       "  'ref_doc_id': 'ea9e88f3a6c897271dcf5506059799dfc02b2f3c',\n",
       "  'relationships': '{\"1\": \"ea9e88f3a6c897271dcf5506059799dfc02b2f3c\"}'}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.Index('ssk')\n",
    "print(index.describe_index_stats())\n",
    "vectorstore = Pinecone.from_existing_index(index_name='ssk', embedding=OpenAIEmbeddings(), namespace='github-llama-index')\n",
    "documents = vectorstore.similarity_search('How can several llama_indexes be composed?')\n",
    "[doc.metadata for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='# Composability\\n\\n\\nLlamaIndex offers **composability** of your indices, meaning that you can build indices on top of other indices. This allows you to more effectively index your entire document tree in order to feed custom knowledge to GPT.\\n\\nComposability allows you to to define lower-level indices for each document, and higher-order indices over a collection of documents. To see how this works, imagine defining 1) a tree index for the text within each document, and 2) a list index over each tree index (one document) within your collection.\\n\\n### Defining Subindices\\nTo see how this works, imagine you have 3 documents: `doc1`, `doc2`, and `doc3`.\\n\\n```python\\ndoc1 = SimpleDirectoryReader(\\'data1\\').load_data()\\ndoc2 = SimpleDirectoryReader(\\'data2\\').load_data()\\ndoc3 = SimpleDirectoryReader(\\'data3\\').load_data()\\n```\\n\\n![](/_static/composability/diagram_b0.png)\\n\\nNow let\\'s define a tree index for each document. In Python, we have:\\n\\n```python\\nindex1 = GPTTreeIndex.from_documents(doc1)\\nindex2 = GPTTreeIndex.from_documents(doc2)\\nindex3 = GPTTreeIndex.from_documents(doc3)\\n```\\n\\n![](/_static/composability/diagram_b1.png)\\n\\n### Defining Summary Text\\n\\nYou then need to explicitly define *summary text* for each subindex. This allows  \\nthe subindices to be used as Documents for higher-level indices.\\n\\n```python\\nindex1_summary = \"<summary1>\"\\nindex2_summary = \"<summary2>\"\\nindex3_summary = \"<summary3>\"\\n```\\n\\nYou may choose to manually specify the summary text, or use LlamaIndex itself to generate\\na summary, for instance with the following:\\n\\n```python\\nsummary = index1.query(\\n    \"What is a summary of this document?\", retriever_mode=\"all_leaf\"\\n)\\nindex1_summary = str(summary)\\n```\\n\\n**If specified**, this summary text for each subindex can be used to refine the answer during query-time. \\n\\n### Creating a Graph with a Top-Level Index\\n\\nWe can then create a graph with a list index on top of these 3 tree indices:\\nWe can query, save, and load the graph to/from disk as any other index.\\n\\n```python\\nfrom llama_index.indices.composability import ComposableGraph\\n\\ngraph = ComposableGraph.from_indices(\\n    GPTListIndex,\\n    [index1, index2, index3],\\n    index_summaries=[index1_summary, index2_summary, index3_summary],\\n)\\n\\n```\\n\\n![](/_static/composability/diagram.png)\\n\\n\\n### Querying the Graph\\n\\nDuring a query, we would start with the top-level list index. Each node in the list corresponds to an underlying tree index. \\nThe query will be executed recursively, starting from the root index, then the sub-indices.\\nThe default query engine for each index is called under the hood (i.e. `index.as_query_engine()`), unless otherwise configured by passing `custom_query_engines` to the `ComposableGraphQueryEngine`.\\nBelow we show an example that configure the tree index retrievers to use `child_branch_factor=2` (instead of the default `child_branch_factor=1`).\\n\\n\\nMore detail on how to configure `ComposableGraphQueryEngine` can be found [here](/reference/query/query_engines/graph_query_engine.rst).\\n\\n\\n```python\\n# set custom retrievers. An example is provided below\\ncustom_query_engines = {\\n    index.index_id: index.as_query_engine(\\n        child_branch_factor=2\\n    ) \\n    for index in [index1, index2, index3]\\n}\\nquery_engine = graph.as_query_engine(\\n    custom_query_engines=custom_query_engines\\n)\\nresponse = query_engine.query(\"Where did the author grow up?\")\\n```\\n\\n> Note that specifying custom retriever for index by id\\n> might require you to inspect e.g., `index1.index_struct.index_id`.\\n> Alternatively, you can explicitly set it as follows:\\n```python\\nindex1.index_struct.index_id = \"<index_id_1>\"\\nindex2.index_struct.index_id = \"<index_id_2>\"\\nindex3.index_struct.index_id = \"<index_id_3>\"\\n```\\n\\n![](/_static/composability/diagram_q1.png)\\n\\nSo within a node, instead of fetching the text, we would recursively query the stored tree index to retrieve our answer.\\n\\n![](/_static/composability/diagram_q2.png)\\n\\nNOTE: You can stack indices as many times as you want, depending on the hierarchies of your knowledge base! \\n\\n\\nWe can take a look at a code example below as well. We first build two tree indices, one over the Wikipedia NYC page, and the other over Paul Graham\\'s essay. We then define a keyword extractor index over the two tree indices.\\n\\n[Here is an example notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/composable_indices/ComposableIndices.ipynb).\\n\\n\\n```{toctree}\\n---\\ncaption: Examples\\nmaxdepth: 1\\n---\\n../../examples/composable_indices/ComposableIndices-Prior.ipynb\\n../../examples/composable_indices/ComposableIndices-Weaviate.ipynb\\n../../examples/composable_indices/ComposableIndices.ipynb\\n```', metadata={'doc_id': 'cc1c9dba6db2572cc3b8db8427880ca8a7240366', 'document_id': 'cc1c9dba6db2572cc3b8db8427880ca8a7240366', 'file_name': 'composability.md', 'file_path': 'docs/how_to/index_structs/composability.md', 'node_info': '{\"_node_type\": \"1\"}', 'ref_doc_id': 'cc1c9dba6db2572cc3b8db8427880ca8a7240366', 'relationships': '{\"1\": \"cc1c9dba6db2572cc3b8db8427880ca8a7240366\"}'})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Several `llama_index` instances can be composed using the `IndexComposer` class. Here\\'s an example:\\n\\n```python\\nfrom llama_index import GPTVectorStoreIndex, IndexComposer\\n\\n# Create the first index\\ndocuments1 = SimpleDirectoryReader(\\'data1\\').load_data()\\nindex1 = GPTVectorStoreIndex.from_documents(documents1)\\n\\n# Create the second index\\ndocuments2 = SimpleDirectoryReader(\\'data2\\').load_data()\\nindex2 = GPTVectorStoreIndex.from_documents(documents2)\\n\\n# Compose the two indexes\\ncomposer = IndexComposer()\\ncomposer.add_index(index1)\\ncomposer.add_index(index2)\\n\\n# Use the composed index for querying\\ncomposed_index = composer.compose_index()\\n\\nquery_engine = composed_index.as_query_engine()\\nresponse = query_engine.query(\"What did the author do growing up?\")\\nprint(response)\\n```\\n\\nIn this example, we create two `GPTVectorStoreIndex` instances (`index1` and `index2`) from different sets of documents (`data1` and `data2`). We then use the `IndexComposer` class to compose the two indexes together into a single index (`composed_index`). Finally, we can use the composed index to perform queries as usual.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='gpt-3.5-turbo-16k')\n",
    "retriever = Pinecone.from_existing_index(index_name='ssk', embedding=OpenAIEmbeddings(), namespace='github-llama-index').as_retriever()\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_reduce\", retriever=retriever, return_source_documents=True)\n",
    "result = qa({\"query\": 'How can several llama_indexes be composed? Show an example.'})\n",
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To compose several `llama_index` instances, you can use the `ComposableGraph` class provided by the LlamaIndex library. Here\\'s an example:\\n\\n```python\\nfrom llama_index import GPTVectorStoreIndex, GPTListIndex\\nfrom llama_index.indices.composability import ComposableGraph\\n\\n# Create the individual indexes\\nindex1 = GPTVectorStoreIndex.from_documents(documents1)\\nindex2 = GPTListIndex.from_documents(documents2)\\n\\n# Compose the indexes into a single graph\\ngraph = ComposableGraph.from_indices(GPTListIndex, [index1, index2], index_summaries=[\"summary1\", \"summary2\"])\\n\\n# Use the composed index for querying\\nquery_engine = graph.as_query_engine()\\nresponse = query_engine.query(\"What is the answer to my question?\")\\nprint(response)\\n```\\n\\nIn this example, `index1` and `index2` are two separate `llama_index` instances representing different sets of documents. The `ComposableGraph.from_indices()` method is used to compose these indexes into a single graph. \\n\\nYou need to provide the type of index to create (`GPTListIndex` in this case) and provide a list of indexes to compose. The `index_summaries` parameter is optional and can be used to provide human-readable summaries for each index.\\n\\nOnce the graph is created, you can create a query engine from the graph using the `as_query_engine()` method. This query engine can then be used to perform queries on the composed index.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = Qdrant(client=qdrant_client.QdrantClient(url='http://localhost:6333'), collection_name='github', embeddings=OpenAIEmbeddings(), content_payload_key='text').as_retriever()\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_reduce\", retriever=retriever, return_source_documents=True)\n",
    "result = qa({\"query\": 'How can several llama_indexes be composed? Show an example.'})\n",
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='# How Each Index Works\\n\\nThis guide describes how each index works with diagrams. We also visually highlight our \"Response Synthesis\" modes.\\n\\nSome terminology:\\n- **Node**: Corresponds to a chunk of text from a Document. LlamaIndex takes in Document objects and internally parses/chunks them into Node objects.\\n- **Response Synthesis**: Our module which synthesizes a response given the retrieved Node. You can see how to \\n    [specify different response modes](setting-response-mode) here. \\n    See below for an illustration of how each response mode works.\\n\\n## List Index\\n\\nThe list index simply stores Nodes as a sequential chain.\\n\\n![](/_static/indices/list.png)\\n\\n### Querying\\n\\nDuring query time, if no other query parameters are specified, LlamaIndex simply loads all Nodes in the list into\\nour Reponse Synthesis module.\\n\\n![](/_static/indices/list_query.png)\\n\\nThe list index does offer numerous ways of querying a list index, from an embedding-based query which \\nwill fetch the top-k neighbors, or with the addition of a keyword filter, as seen below:\\n\\n![](/_static/indices/list_filter_query.png)\\n\\n\\n## Vector Store Index\\n\\nThe vector store index stores each Node and a corresponding embedding in a [Vector Store](vector-store-index).\\n\\n![](/_static/indices/vector_store.png)\\n\\n### Querying\\n\\nQuerying a vector store index involves fetching the top-k most similar Nodes, and passing\\nthose into our Response Synthesis module.\\n\\n![](/_static/indices/vector_store_query.png)\\n\\n## Tree Index\\n\\nThe tree index builds a hierarchical tree from a set of Nodes (which become leaf nodes in this tree).\\n\\n![](/_static/indices/tree.png)\\n\\n### Querying\\n\\nQuerying a tree index involves traversing from root nodes down \\nto leaf nodes. By default, (`child_branch_factor=1`), a query\\nchooses one child node given a parent node. If `child_branch_factor=2`, a query\\nchooses two child nodes per parent.\\n\\n![](/_static/indices/tree_query.png)\\n\\n## Keyword Table Index\\n\\nThe keyword table index extracts keywords from each Node and builds a mapping from \\neach keyword to the corresponding Nodes of that keyword.\\n\\n![](/_static/indices/keyword.png)\\n\\n### Querying\\n\\nDuring query time, we extract relevant keywords from the query, and match those with pre-extracted\\nNode keywords to fetch the corresponding Nodes. The extracted Nodes are passed to our \\nResponse Synthesis module.\\n\\n![](/_static/indices/keyword_query.png)\\n\\n## Response Synthesis\\n\\nLlamaIndex offers different methods of synthesizing a response. The way to toggle this can be found in our \\n[Usage Pattern Guide](setting-response-mode). Below, we visually highlight how each response mode works.\\n\\n### Create and Refine\\n\\nCreate and refine is an iterative way of generating a response. We first use the context in the first node, along\\nwith the query, to generate an initial answer. We then pass this answer, the query, and the context of the second node\\nas input into a \"refine prompt\" to generate a refined answer. We refine through N-1 nodes, where N is the total \\nnumber of nodes.\\n\\n![](/_static/indices/create_and_refine.png)\\n\\n### Tree Summarize\\n\\nTree summarize is another way of generating a response. We essentially build a tree index\\nover the set of candidate nodes, with a *summary prompt* seeded with the query. The tree\\nis built in a bottoms-up fashion, and in the end the root node is returned as the response.\\n\\n![](/_static/indices/tree_summarize.png)\\n', metadata={})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['source_documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
